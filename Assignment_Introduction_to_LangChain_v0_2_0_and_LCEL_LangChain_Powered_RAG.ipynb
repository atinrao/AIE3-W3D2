{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47eTBHYNP4g1"
      },
      "source": [
        "# Introduction to LangChain v0.2.0 and LCEL: LangChain Powered RAG\n",
        "\n",
        "In the following notebook we're going to focus on learning how to navigate and build useful applications using LangChain, specifically LCEL, and how to integrate different APIs together into a coherent RAG application!\n",
        "\n",
        "In the notebook, you'll complete the following Tasks:\n",
        "\n",
        "- ðŸ¤ Breakout Room #1:\n",
        "  1. Install required libraries\n",
        "  2. Set Environment Variables\n",
        "  3. Initialize a Simple Chain using LCEL\n",
        "  4. Implement Naive RAG using LCEL\n",
        "\n",
        "- ðŸ¤ Breakout Room #2:\n",
        "  1. Create a Simple RAG Application Using QDrant, OpenAI, and LCEL\n",
        "\n",
        "Let's get started!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ayVXHXHRE_t"
      },
      "source": [
        "# ðŸ¤ Breakout Room #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVHd6POM0JFN"
      },
      "source": [
        "## Task 1: Installing Required Libraries\n",
        "\n",
        "One of the [key features](https://blog.langchain.dev/langchain-v02-leap-to-stability/) of LangChain v0.2.0 is the compartmentalization of the various LangChain ecosystem packages and added stability.\n",
        "\n",
        "Instead of one all encompassing Python package - LangChain has a `core` package and a number of additional supplementary packages.\n",
        "\n",
        "We'll start by grabbing all of our LangChain related packages!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCC2AR-Q0m0x",
        "outputId": "a9936260-6134-4294-f21c-6a40111a57ca"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain langchain-core langchain-community langchain-openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5ELHQjQ1PYs"
      },
      "source": [
        "Now we can get our Qdrant dependencies!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76XeYI9P1OXO",
        "outputId": "20cf9b81-95bd-4ebf-ae3a-cf0f5912f3eb"
      },
      "outputs": [],
      "source": [
        "!pip install -qU qdrant-client"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iesey9OGCKJx"
      },
      "source": [
        "Let's finally get `tiktoken` and `pymupdf` so we can leverage them later on!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5qIUrFuENrS",
        "outputId": "61571142-eb50-4eaa-ac68-4c39062a36ad"
      },
      "outputs": [],
      "source": [
        "!pip install -qU tiktoken pymupdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sl6wTp9C5qbY"
      },
      "source": [
        "## Task 2: Set Environment Variables\n",
        "\n",
        "We'll be leveraging OpenAI's suite of APIs - so we'll set our `OPENAI_API_KEY` `env` variable here!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pKAfycq73wE",
        "outputId": "404522d0-c907-44bc-c8f8-f72084dfbb57"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_xp54wIA56_"
      },
      "source": [
        "## Task 3: Initialize a Simple Chain using LCEL\n",
        "\n",
        "The first thing we'll do is familiarize ourselves with LCEL and the specific ins and outs of how we can use it!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyGdhbS6SkD1"
      },
      "source": [
        "### LLM Orchestration Tool (LangChain)\n",
        "\n",
        "Let's dive right into [LangChain](https://www.langchain.com/)!\n",
        "\n",
        "The first thing we want to do is create an object that lets us access OpenAI's `gpt-40` model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3Uj6SorxMj8e"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "openai_chat_model = ChatOpenAI(model=\"gpt-4o\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsmiieEh_Ye-"
      },
      "source": [
        "####â“ Question #1:\n",
        "\n",
        "What other models could we use, and how would the above code change?\n",
        "\n",
        "Answer: There are lot of models which can use e.g., GPT 4 turbo, GPT 4, GPT 3.5 Turbo, Dalle, TTS, Whisper etc.\n",
        "To use a different model, we have to specify its name as a parameter for ChatOpenAI e.g., ChatOpenAI(model=\"gpt-4-turbo\")\n",
        "\n",
        "> HINT: Check out [this page](https://platform.openai.com/docs/models/gpt-3-5-turbo) to find the answer!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nU8SlHfH41T"
      },
      "source": [
        "### Prompt Template"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcMKLZWBVYm7"
      },
      "source": [
        "Now, we'll set up a prompt template - more specifically a `ChatPromptTemplate`. This will let us build a prompt we can modify when we call our LLM!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Z770j4zPS3o5"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "system_template = \"You are a politician. You speak in an ambiguous manner.\"\n",
        "human_template = \"{content}\"\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", system_template),\n",
        "    (\"human\", human_template)\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGku_c2VVyd_"
      },
      "source": [
        "### Our First Chain\n",
        "\n",
        "Now we can set up our first chain!\n",
        "\n",
        "A chain is simply two components that feed directly into eachother in a sequential fashion!\n",
        "\n",
        "You'll notice that we're using the pipe operator `|` to connect our `chat_prompt` to our `llm`.\n",
        "\n",
        "This is a simplified method of creating chains and it leverages the LangChain Expression Language, or LCEL.\n",
        "\n",
        "You can read more about it [here](https://python.langchain.com/docs/expression_language/), but there a few features we should be aware of out of the box (taken directly from LangChain's documentation linked above):\n",
        "\n",
        "- **Async, Batch, and Streaming Support** Any chain constructed this way will automatically have full sync, async, batch, and streaming support. This makes it easy to prototype a chain in a Jupyter notebook using the sync interface, and then expose it as an async streaming interface.\n",
        "\n",
        "- **Fallbacks** The non-determinism of LLMs makes it important to be able to handle errors gracefully. With LCEL you can easily attach fallbacks to any chain.\n",
        "\n",
        "- **Parallelism** Since LLM applications involve (sometimes long) API calls, it often becomes important to run things in parallel. With LCEL syntax, any components that can be run in parallel automatically are.\n",
        "\n",
        "In the following code cell we have two components:\n",
        "\n",
        "- `chat_prompt`, which is a formattable `ChatPromptTemplate` that contains a system message and a human message.\n",
        "\n",
        "We'd like to be able to pass our own `content` (as found in our `human_template`) and then have the resulting message pair sent to our model and responded to!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "RcJyqOiwVt04"
      },
      "outputs": [],
      "source": [
        "chain = chat_prompt | openai_chat_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QV_kHCjlL_01"
      },
      "source": [
        "Notice the pattern here:\n",
        "\n",
        "We invoke our chain with the `dict` `{\"content\" : \"Hello world!\"}`.\n",
        "\n",
        "It enters our chain:\n",
        "\n",
        "`{\"content\" : \"Hello world!\"}` -> `invoke()` -> `chat_prompt`\n",
        "\n",
        "Our `chat_prompt` returns a `PromptValue`, which is the formatted prompt. We then \"pipe\" the output of our `chat_prompt` into our `llm`.\n",
        "\n",
        "`PromptValue` -> `|` -> `llm`\n",
        "\n",
        "Our `llm` then takes the list of messages and provides an output which is return as a `str`!\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cqr2QuMtIjn",
        "outputId": "7c8655d5-c39e-42dd-8766-c784026b62ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content=\"Greetings! It's always a pleasure to connect with the vibrant and diverse community out there. The world is indeed full of possibilities and opportunities. How can I assist you today?\" response_metadata={'token_usage': {'completion_tokens': 34, 'prompt_tokens': 26, 'total_tokens': 60}, 'model_name': 'gpt-4o', 'system_fingerprint': 'fp_319be4768e', 'finish_reason': 'stop', 'logprobs': None} id='run-2f885536-c7fb-42be-a822-dada47b3c0c6-0' usage_metadata={'input_tokens': 26, 'output_tokens': 34, 'total_tokens': 60}\n"
          ]
        }
      ],
      "source": [
        "print(chain.invoke({\"content\": \"Hello world!\"}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2znL48ECNteM"
      },
      "source": [
        "Let's try it out with a different prompt!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjiTNeYXUCAB",
        "outputId": "840e9d31-7387-48f3-cbb9-7a3495985698"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Green energy is a topic of great significance and one that encompasses a diverse array of perspectives and considerations. It is clear that there are numerous potential benefits associated with the advancement of green energy initiatives, including environmental, economic, and societal impacts. However, it is also important to acknowledge the complexities and challenges that may arise as we transition towards more sustainable energy sources. Striking the right balance between innovation, practicality, and inclusivity will be key as we navigate this critical issue.', response_metadata={'token_usage': {'completion_tokens': 94, 'prompt_tokens': 31, 'total_tokens': 125}, 'model_name': 'gpt-4o', 'system_fingerprint': 'fp_aa87380ac5', 'finish_reason': 'stop', 'logprobs': None}, id='run-3901e206-39c7-4d7c-944a-dbd854f80a52-0', usage_metadata={'input_tokens': 31, 'output_tokens': 94, 'total_tokens': 125})"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.invoke({\"content\" : \"What is your take on green energy?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THcMz8YAWsjP"
      },
      "source": [
        "Notice how we specifically referenced our `content` format option!\n",
        "\n",
        "Now that we have the basics set up - let's see what we mean by \"Retrieval Augmented\" Generation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7o8aXbhRAPe"
      },
      "source": [
        "## Naive RAG - Manually adding context through the Prompt Template\n",
        "\n",
        "Let's look at how our model performs at a simple task - defining what LangChain is!\n",
        "\n",
        "We'll redo some of our previous work to change the `system_template` to be less...verbose."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qu_Uox_pPKaf",
        "outputId": "5b93eef2-c844-4a23-cd87-94b1686a943a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content=\"LangChain is a framework designed to help developers create applications using large language models (LLMs). It offers modular components that can be combined to build various applications, such as chatbots, Generative Question-Answering (GQA) systems, summarization tools, and more. The framework focuses on enabling applications that:\\n\\n1. **Understand Context**: LangChain allows applications to maintain and leverage context, which is crucial for generating relevant and coherent responses.\\n2. **Interact with Data**: It enables the integration of language models with external data sources, allowing applications to perform tasks that require real-time data access.\\n3. **Compose Components**: LangChain provides a set of building blocks that can be assembled in different ways to create diverse applications. These components include prompt templates, memory modules, and various chains for processing inputs and outputs.\\n\\nLangChain is particularly useful for developers who want to build sophisticated language model-driven applications without needing to start from scratch. It abstracts many complex aspects of working with LLMs, making it easier to focus on the application's specific functionality and user experience.\" response_metadata={'token_usage': {'completion_tokens': 217, 'prompt_tokens': 22, 'total_tokens': 239}, 'model_name': 'gpt-4o', 'system_fingerprint': 'fp_319be4768e', 'finish_reason': 'stop', 'logprobs': None} id='run-31ca5e17-5cb3-4d2a-ac29-e9f08e02d404-0' usage_metadata={'input_tokens': 22, 'output_tokens': 217, 'total_tokens': 239}\n"
          ]
        }
      ],
      "source": [
        "system_template = \"You are a helpful assistant.\"\n",
        "human_template = \"{content}\"\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", system_template),\n",
        "    (\"human\", human_template)\n",
        "])\n",
        "\n",
        "chat_chain = chat_prompt | openai_chat_model\n",
        "\n",
        "print(chat_chain.invoke({\"content\" : \"Please define LangChain.\"}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18KXqGI4XbMb"
      },
      "source": [
        "Well, that's not very good - is it!\n",
        "\n",
        "The issue at play here is that our model was not trained on the idea of \"LangChain\", and so it's left with nothing but a guess - definitely not what we want the answer to be!\n",
        "\n",
        "Let's ask another simple LangChain question!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRG5LwYoXnsr",
        "outputId": "58769a17-aba2-45f8-e9d4-23e96314de54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content='LangChain Expression Language (LCEL) is a specialized expression language designed for use with the LangChain framework, which is a toolkit for developing applications that integrate with various language models and data sources. LCEL enables developers to define expressions and logic within their LangChain applications, facilitating more complex and dynamic interactions with language models.\\n\\nLCEL provides a way to create, manipulate, and evaluate expressions that can be used to control the flow of data, perform calculations, and implement conditional logic within LangChain workflows. This allows for more sophisticated and customized behavior in applications that leverage language models, enhancing their capability to process and respond to user inputs and other data in a meaningful way.\\n\\nIn summary, LCEL is a powerful tool within the LangChain ecosystem that helps developers build more advanced and customized language model applications by providing a structured way to define and evaluate expressions and logic.' response_metadata={'token_usage': {'completion_tokens': 171, 'prompt_tokens': 27, 'total_tokens': 198}, 'model_name': 'gpt-4o', 'system_fingerprint': 'fp_f4e629d0a5', 'finish_reason': 'stop', 'logprobs': None} id='run-c3d4607d-3b4b-4f5d-a71f-d151a063e6dc-0' usage_metadata={'input_tokens': 27, 'output_tokens': 171, 'total_tokens': 198}\n"
          ]
        }
      ],
      "source": [
        "print(chat_chain.invoke({\"content\" : \"What is LangChain Expression Language (LCEL)?\"}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63pr0fgYXxC3"
      },
      "source": [
        "While it provides a confident response, that response is entirely ficticious! Not a great look, OpenAI!\n",
        "\n",
        "However, let's see what happens when we rework our prompts - and we add the content from the docs to our prompt as context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fgr25HjgYHwh",
        "outputId": "7d590389-8a46-4105-fbe7-c08495dced25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content='LangChain Expression Language (LCEL) is a declarative way to easily compose chains together. It offers several benefits, such as full sync, async, batch, and streaming support, easy attachment of fallbacks to handle errors gracefully, automatic parallel execution of components that can be run in parallel, and seamless integration with LangSmith for enhanced observability and debuggability.' response_metadata={'token_usage': {'completion_tokens': 74, 'prompt_tokens': 274, 'total_tokens': 348}, 'model_name': 'gpt-4o', 'system_fingerprint': 'fp_319be4768e', 'finish_reason': 'stop', 'logprobs': None} id='run-d052b4bb-9fb8-42dd-87be-1c41effbb3d6-0' usage_metadata={'input_tokens': 274, 'output_tokens': 74, 'total_tokens': 348}\n"
          ]
        }
      ],
      "source": [
        "HUMAN_TEMPLATE = \"\"\"\n",
        "#CONTEXT:\n",
        "{context}\n",
        "\n",
        "QUERY:\n",
        "{query}\n",
        "\n",
        "Use the provide context to answer the provided user query. Only use the provided context to answer the query. If you do not know the answer, response with \"I don't know\"\n",
        "\"\"\"\n",
        "\n",
        "CONTEXT = \"\"\"\n",
        "LangChain Expression Language or LCEL is a declarative way to easily compose chains together. There are several benefits to writing chains in this manner (as opposed to writing normal code):\n",
        "\n",
        "Async, Batch, and Streaming Support Any chain constructed this way will automatically have full sync, async, batch, and streaming support. This makes it easy to prototype a chain in a Jupyter notebook using the sync interface, and then expose it as an async streaming interface.\n",
        "\n",
        "Fallbacks The non-determinism of LLMs makes it important to be able to handle errors gracefully. With LCEL you can easily attach fallbacks to any chain.\n",
        "\n",
        "Parallelism Since LLM applications involve (sometimes long) API calls, it often becomes important to run things in parallel. With LCEL syntax, any components that can be run in parallel automatically are.\n",
        "\n",
        "Seamless LangSmith Tracing Integration As your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step. With LCEL, all steps are automatically logged to LangSmith for maximal observability and debuggability.\n",
        "\"\"\"\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"human\", HUMAN_TEMPLATE)\n",
        "])\n",
        "\n",
        "chat_chain = chat_prompt | openai_chat_model\n",
        "\n",
        "print(chat_chain.invoke({\"query\" : \"What is LangChain Expression Language?\", \"context\" : CONTEXT}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppQdtCedY7C4"
      },
      "source": [
        "You'll notice that the response is much better this time. Not only does it answer the question well - but there's no trace of confabulation (hallucination) at all!\n",
        "\n",
        "> NOTE: While RAG is an effective strategy to *help* ground LLMs, it is not nearly 100% effective. You will still need to ensure your responses are factual through some other processes\n",
        "\n",
        "That, in essence, is the idea of RAG. We provide the model with context to answer our queries - and rely on it to translate the potentially lengthy and difficult to parse context into a natural language answer!\n",
        "\n",
        "However, manually providing context is not scalable - and doesn't really offer any benefit.\n",
        "\n",
        "Enter: Retrieval Pipelines."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFmdARsVBJUq"
      },
      "source": [
        "## Task #4: Implement Naive RAG using LCEL\n",
        "\n",
        "Now we can make a naive RAG application that will help us bridge the gap between our Pythonic implementation and a fully LangChain powered solution!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4AozVoEZveK"
      },
      "source": [
        "## Putting the R in RAG: Retrieval 101\n",
        "\n",
        "In order to make our RAG system useful, we need a way to provide context that is most likely to answer our user's query to the LLM as additional context.\n",
        "\n",
        "Let's tackle an immediate problem first: The Context Window.\n",
        "\n",
        "All (most) LLMs have a limited context window which is typically measured in tokens. This window is an upper bound of how much stuff we can stuff in the model's input at a time.\n",
        "\n",
        "Let's say we want to work off of a relatively large piece of source data - like the Ultimate Hitchhiker's Guide to the Galaxy. All 898 pages of it!\n",
        "\n",
        "> NOTE: It is recommended you do not run the following cells, they are purely for demonstrative purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "PbXBxffibeyp"
      },
      "outputs": [],
      "source": [
        "context = \"\"\"\n",
        "EVERY HITCHHIKER'S GUIDE BOOK\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZvgFuaXcHFT"
      },
      "source": [
        "We can leverage our tokenizer to count the number of tokens for us!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "HaKPOdSjbifn"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "\n",
        "enc = tiktoken.encoding_for_model(\"gpt-4o\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtDiSMxpE4Xi",
        "outputId": "886fd517-9128-45ca-9fbd-5152ae7f146b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(enc.encode(context))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oUuZpAicLdm"
      },
      "source": [
        "The full set comes in at a whopping *636,144* tokens.\n",
        "\n",
        "So, we have too much context. What can we do?\n",
        "\n",
        "Well, the first thing that might enter your mind is: \"Use a model with more context window\", and we could definitely do that! However, even `gpt-4-32k` wouldn't be able to fit that whole text in the context window at once.\n",
        "\n",
        "So, we can try splitting our document up into little pieces - that way, we can avoid providing too much context.\n",
        "\n",
        "We have another problem now.\n",
        "\n",
        "If we split our document up into little pieces, and we can't put all of them in the prompt. How do we decide which to include in the prompt?!\n",
        "\n",
        "> NOTE: Content splitting/chunking strategies are an active area of research and iterative developement. There is no \"one size fits all\" approach to chunking/splitting at this moment. Use your best judgement to determine chunking strategies!\n",
        "\n",
        "In order to conceptualize the following processes - let's create a toy context set!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPCiOPwUfbqn"
      },
      "source": [
        "### TextSplitting aka Chunking\n",
        "\n",
        "We'll use the `RecursiveCharacterTextSplitter` to create our toy example.\n",
        "\n",
        "It will split based on the following rules:\n",
        "\n",
        "- Each chunk has a maximum size of 100 tokens\n",
        "- It will try and split first on the `\\n\\n` character, then on the `\\n`, then on the `<SPACE>` character, and finally it will split on individual tokens.\n",
        "\n",
        "Let's implement it and see the results!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "nLW9AfDKfVHn"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "def tiktoken_len(text):\n",
        "    tokens = tiktoken.encoding_for_model(\"gpt-4o\").encode(\n",
        "        text,\n",
        "    )\n",
        "    return len(tokens)\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 100,\n",
        "    chunk_overlap = 0,\n",
        "    length_function = tiktoken_len,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "nPYPBe2ngT9N"
      },
      "outputs": [],
      "source": [
        "chunks = text_splitter.split_text(CONTEXT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_RGVlTihaQX",
        "outputId": "d1b260d2-8559-4ccc-d333-9e66543d9ed2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTYny2xchS_Z",
        "outputId": "35a390f4-2e17-46f6-b4d2-7c9c86773507"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LangChain Expression Language or LCEL is a declarative way to easily compose chains together. There are several benefits to writing chains in this manner (as opposed to writing normal code):\n",
            "\n",
            "Async, Batch, and Streaming Support Any chain constructed this way will automatically have full sync, async, batch, and streaming support. This makes it easy to prototype a chain in a Jupyter notebook using the sync interface, and then expose it as an async streaming interface.\n",
            "470\n",
            "90\n",
            "----\n",
            "Fallbacks The non-determinism of LLMs makes it important to be able to handle errors gracefully. With LCEL you can easily attach fallbacks to any chain.\n",
            "\n",
            "Parallelism Since LLM applications involve (sometimes long) API calls, it often becomes important to run things in parallel. With LCEL syntax, any components that can be run in parallel automatically are.\n",
            "358\n",
            "75\n",
            "----\n",
            "Seamless LangSmith Tracing Integration As your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step. With LCEL, all steps are automatically logged to LangSmith for maximal observability and debuggability.\n",
            "273\n",
            "52\n",
            "----\n"
          ]
        }
      ],
      "source": [
        "for chunk in chunks:\n",
        "  print(chunk)\n",
        "  print(len(chunk))\n",
        "  print(len(enc.encode(chunk)))\n",
        "  print(\"----\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98hOgu5Yhefv"
      },
      "source": [
        "As is shown in our result, we've split each section into 100 token chunks - cleanly separated by `\\n\\n` characters!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PTiJ2utMpqq"
      },
      "source": [
        "####ðŸ—ï¸ Activity #1:\n",
        "\n",
        "While there's nothing specifically wrong with the chunking method used above - it is a naive approach that is not sensitive to specific data formats.\n",
        "\n",
        "Brainstorm some ideas that would split large single documents into smaller documents.\n",
        "\n",
        "1. Paragraph chuking i.e., each paragraph becomes its own chunk. This can be useful in scenarios where each paragraph conveys a different meaning and document has a fixed set of predefined paragraphs\n",
        "2. Section chunking i.e., each section in documen becomes its own chunk. A section can be a collection of one or more paragraphs. Generally speaking sections carry semantic meaning, and such a chuking strategy helps to capture semantics.\n",
        "3. Sentence chunking\n",
        "4.Fixed length chunking\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rj18Rjafhp7d"
      },
      "source": [
        "## Embeddings and Dense Vector Search\n",
        "\n",
        "Now that we have our individual chunks, we need a system to correctly select the relevant pieces of information to answer our query.\n",
        "\n",
        "This sounds like a perfect job for embeddings!\n",
        "\n",
        "We'll be using OpenAI's `text-embedding-3` model as our embedding model today!\n",
        "\n",
        "Let's load it up through LangChain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "quNjOLWspOVN"
      },
      "outputs": [],
      "source": [
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "\n",
        "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsGZ92hm9IeX"
      },
      "source": [
        "####â“ Question #2:\n",
        "\n",
        "What is the embedding dimension, given that we're using `text-embedding-3-small`?\n",
        "\n",
        "Answer: Embedding dimension for 'text-embedding-3-small' is 1536\n",
        "\n",
        "> HINT: Check out the [docs](https://platform.openai.com/docs/guides/embeddings) to help you answer this question."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByK-zb0FsnqR"
      },
      "source": [
        "### Finding the Embeddings for Our Chunks\n",
        "\n",
        "First, let's find all our embeddings for each chunk and store them in a convenient format for later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ZHl-u6Fxske9"
      },
      "outputs": [],
      "source": [
        "embeddings_dict = {}\n",
        "\n",
        "for chunk in chunks:\n",
        "  embeddings_dict[chunk] = embedding_model.embed_query(chunk)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhJ9wY2etK7y",
        "outputId": "bc3554eb-9075-4a7f-c028-7dae06e7256c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chunk - LangChain Expression Language or LCEL is a declarative way to easily compose chains together. There are several benefits to writing chains in this manner (as opposed to writing normal code):\n",
            "\n",
            "Async, Batch, and Streaming Support Any chain constructed this way will automatically have full sync, async, batch, and streaming support. This makes it easy to prototype a chain in a Jupyter notebook using the sync interface, and then expose it as an async streaming interface.\n",
            "---\n",
            "Embedding - Vector of Size: 1536\n",
            "\n",
            "\n",
            "\n",
            "Chunk - Fallbacks The non-determinism of LLMs makes it important to be able to handle errors gracefully. With LCEL you can easily attach fallbacks to any chain.\n",
            "\n",
            "Parallelism Since LLM applications involve (sometimes long) API calls, it often becomes important to run things in parallel. With LCEL syntax, any components that can be run in parallel automatically are.\n",
            "---\n",
            "Embedding - Vector of Size: 1536\n",
            "\n",
            "\n",
            "\n",
            "Chunk - Seamless LangSmith Tracing Integration As your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step. With LCEL, all steps are automatically logged to LangSmith for maximal observability and debuggability.\n",
            "---\n",
            "Embedding - Vector of Size: 1536\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for k,v in embeddings_dict.items():\n",
        "  print(f\"Chunk - {k}\")\n",
        "  print(\"---\")\n",
        "  print(f\"Embedding - Vector of Size: {len(v)}\")\n",
        "  print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOxYybdNtkWv"
      },
      "source": [
        "Okay, great. Let's create a query - and then embed it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQD5Zwl1tLrZ",
        "outputId": "d42d7488-edca-458b-dca4-1b11f3536cb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.0009562299237586558, -0.034175533801317215, 0.022653907537460327, -0.03587709739804268, 0.021269582211971283, 0.009171155281364918, 0.006600780412554741, 0.02275484800338745, -0.0017790744313970208, -0.02953227423131466, 0.01599184237420559, -0.01757804863154888, -0.026936663314700127, -0.039972394704818726, -0.027484625577926636, -0.01874607242643833, 0.06483256816864014, 0.001774568110704422, -0.07019682973623276, 0.043865811079740524, 0.017477108165621758, 0.03169528394937515, 0.022769268602132797, 0.006060028448700905, -0.031926002353429794, -0.019827576354146004, -0.033396847546100616, 0.012776168994605541, 0.003705954411998391, -0.044183049350976944, -0.005014574620872736, -0.02847960963845253, -0.06079495698213577, -0.015732280910015106, -0.03504073619842529, -0.018082749098539352, 0.0860012099146843, 0.012422878295183182, 0.03348337113857269, 0.05098931863903999, -0.0035815814044326544, -0.0035401235800236464, -0.004282756708562374, -0.005800467450171709, -0.04770154505968094, 0.01246613822877407, -0.020750461146235466, -0.015746701508760452, 0.01899121329188347, 0.040404997766017914, -0.005991533398628235, -0.012610338628292084, 0.018645131960511208, 0.017996229231357574, -0.04080875962972641, 0.00011040355457225814, -0.020260177552700043, 0.04946079105138779, 0.01147836446762085, -0.01633792370557785, 0.02520625665783882, -0.025999359786510468, -0.012098426930606365, 0.008248272351920605, -0.017030086368322372, 0.01929403468966484, -0.03769402578473091, -0.018962373957037926, -0.00844294298440218, 0.010000308975577354, -0.045538537204265594, 0.007289338391274214, -0.006279934197664261, -0.03887647017836571, -0.009452346712350845, -0.016972405835986137, -0.007923820987343788, 0.013461122289299965, 0.04891283065080643, -0.018385570496320724, 0.006110498681664467, -0.006918021943420172, -0.006406109780073166, 0.011096232570707798, 0.030512837693095207, -0.0494031123816967, -0.0727347582578659, -0.010411280207335949, -0.041414398699998856, -0.05029715597629547, -0.019279614090919495, 0.015818800777196884, -0.022509707137942314, 0.05194104090332985, 0.07250404357910156, 0.10670841485261917, 0.019697796553373337, -0.009776798076927662, 0.02552349865436554, 0.03902067244052887, -0.025537919253110886, -0.011492784135043621, 0.015169898979365826, -0.05280624330043793, 0.034608133137226105, 0.01058432087302208, 0.004041220527142286, -0.023634470999240875, -0.03613666072487831, -0.02481691539287567, -0.05269088223576546, -0.039309073239564896, 0.027787446975708008, -0.020043877884745598, 0.0529216043651104, -0.03342568874359131, 0.005346235819160938, -0.0194526556879282, -0.016871465370059013, -0.0019160648807883263, -0.04605765640735626, -0.011016923002898693, -0.0022080710623413324, -0.03313728794455528, -0.006121313665062189, -0.012869900092482567, 0.02676362358033657, -0.03515609726309776, 0.023129768669605255, -0.05072975531220436, -0.002350469119846821, -0.020375538617372513, 0.03299308940768242, 0.021327262744307518, -0.03815546631813049, 0.0030570519156754017, -0.06546705216169357, -0.026114720851182938, -0.022235725075006485, 0.027513466775417328, 0.017145445570349693, 0.01863071136176586, 0.013352971524000168, 0.025811899453401566, -0.04579809680581093, -0.01906331442296505, -0.022740427404642105, 0.012184946797788143, 0.007080247160047293, 0.0068747615441679955, 0.02325955033302307, -0.003705954411998391, -0.052575524896383286, -0.02506205625832081, -0.015804382041096687, 0.0018241370562463999, -0.003619434079155326, 0.009502816945314407, 0.0005069550825282931, -0.03498305380344391, 0.007462379056960344, 0.0840400829911232, -0.005371470935642719, 0.011593724600970745, 0.011528834700584412, -0.0165398046374321, -0.044644493609666824, -0.01212005689740181, -0.06292912364006042, -0.008601563051342964, -0.007249683141708374, -0.0020836980547755957, 0.013345761224627495, -0.004982129205018282, 0.024715974926948547, -0.052892763167619705, 0.01289873942732811, -0.015544820576906204, -0.025696540251374245, -0.041991204023361206, -0.05929527059197426, 0.0016483926447108388, -0.041183680295944214, 0.017765508964657784, -0.04121251776814461, 0.04377928748726845, -0.0006962183397263288, 0.0365692600607872, -0.04150092229247093, -0.03186832368373871, -0.04680750146508217, 0.019741056486964226, 0.03749214485287666, 0.010389650240540504, -0.013540432788431644, 0.006207833997905254, 0.08663569390773773, -0.02139936201274395, 0.023389330133795738, -0.0315222404897213, -0.010706891305744648, 0.0836363211274147, 0.030801238492131233, 0.00969027727842331, 0.03671346232295036, 0.03380060940980911, -0.05439244955778122, -0.02941691316664219, -0.003915044944733381, -0.03922255337238312, 0.014751717448234558, 0.02504763752222061, -0.020289018750190735, 0.02813352830708027, -0.007563319057226181, 0.005014574620872736, 0.02559559978544712, 0.0323009267449379, -0.0031868324149399996, 0.035502176731824875, 0.012985260225832462, -0.016741685569286346, 0.030253276228904724, -0.0008278013556264341, -0.021543562412261963, 0.03887647017836571, 0.006564730312675238, -0.046865180134773254, 0.041097160428762436, -0.03642506152391434, 0.026979925110936165, -0.021731024608016014, -0.025898419320583344, -0.00172409787774086, 0.041183680295944214, 0.014463315717875957, -0.012610338628292084, -0.041183680295944214, 0.025552337989211082, -0.034723494201898575, -0.020130397751927376, -0.018702812492847443, 0.014484945684671402, 0.007224448025226593, 0.04807646572589874, 0.019193094223737717, -0.04144323989748955, -0.04623069986701012, 0.018529770895838737, 0.0012644586386159062, -0.00439451215788722, -0.0008692590054124594, 0.06033351272344589, -0.05470969155430794, -0.020865820348262787, -0.024831335991621017, 0.01126206386834383, -0.04741314426064491, 0.008976484648883343, -0.037405624985694885, -0.009820058010518551, 0.023389330133795738, -0.047759223729372025, -0.01793854869902134, 0.03475233539938927, -0.012321937829256058, 0.03466581553220749, 0.004354856908321381, -0.00861598365008831, 0.050210632383823395, 0.04879746958613396, -0.01860187202692032, 0.014340745285153389, 0.00814733188599348, -0.019870836287736893, -0.05274856463074684, -0.029820675030350685, 0.01805390976369381, -0.006222254130989313, 0.037838228046894073, -0.07832974195480347, 0.010086828842759132, -0.013057360425591469, -0.020562998950481415, -0.006193413864821196, 0.0057716271840035915, -0.020216917619109154, 0.014794977381825447, -0.02116864174604416, -0.061025675386190414, 0.03956863284111023, 0.03867458924651146, 0.03521377593278885, -0.0036176315043121576, 0.0019178674556314945, -0.0036104214377701283, 0.04236612468957901, 0.022769268602132797, -0.012458927929401398, 0.004942473955452442, 0.03682882338762283, 0.042971767485141754, -0.023043248802423477, -0.028998732566833496, -0.05543069541454315, -0.0003273803158663213, -0.051998719573020935, -0.01773666776716709, 0.024485254660248756, 0.02571095898747444, -0.017866449430584908, 0.005494041368365288, 0.05718994140625, 0.008868333883583546, -0.015213158912956715, -0.07019682973623276, -0.013453911989927292, -0.01949591562151909, 0.037953585386276245, -0.05119119957089424, 0.014037923887372017, -0.08034855127334595, 0.016352342441678047, 0.005274135619401932, -0.026013780385255814, 0.022999988868832588, 0.010057988576591015, -0.003900625044479966, 0.029676474630832672, -0.008161751553416252, -0.010050779208540916, -0.03870343044400215, 0.00955328717827797, 0.0035635563544929028, 0.007830089889466763, -0.01668400503695011, -0.015025698579847813, -0.017520368099212646, 0.011673035100102425, 0.0012320135720074177, -0.03408901020884514, 0.03714606538414955, -0.024528514593839645, 0.009488396346569061, 0.010519430972635746, 0.01246613822877407, 0.004116925876587629, 0.022235725075006485, 0.026720363646745682, -0.021428203210234642, -0.038299668580293655, -0.03948211297392845, -0.0038934149779379368, -0.022351086139678955, 0.03599245846271515, 0.00875297375023365, -0.026778044179081917, -0.06119871512055397, 0.027297165244817734, -0.019712217152118683, -0.01906331442296505, 0.012783379293978214, 0.03801126778125763, 0.019899677485227585, 0.014744507148861885, 0.059929750859737396, -0.004697333090007305, 0.0002341005892958492, 0.009445136412978172, -0.013179930858314037, 0.0696200281381607, -0.015040118247270584, 0.009841687977313995, -0.00035757230944000185, -0.07752221822738647, -0.0022170834708958864, -0.016193723306059837, 0.02964763529598713, 0.0006407912005670369, 0.012401248328387737, -0.01437679585069418, -0.012278677895665169, 0.018299050629138947, 0.013994663953781128, 0.010843882337212563, 0.059814389795064926, 0.010173349641263485, 0.0025325221940875053, -0.030282117426395416, 0.014420055784285069, -0.001022922690026462, -0.05620937794446945, -0.01221378706395626, -0.02190406434237957, 0.03509841486811638, 0.03201252222061157, 0.016193723306059837, 0.007772410288453102, 0.030599357560276985, 0.0013897328171879053, 0.012877109460532665, -0.0369441844522953, 0.01100971270352602, -0.027527887374162674, 0.05041251331567764, -0.02178870514035225, 0.02708086557686329, 0.027873968705534935, -0.03769402578473091, -0.010865512304008007, 0.03659810125827789, -0.013432282023131847, 0.050672076642513275, -0.039309073239564896, 0.0231441892683506, -0.061371758580207825, -0.0019485100638121367, 0.009149525314569473, -0.0028299358673393726, 0.05753602087497711, -0.01578996144235134, -0.04767270386219025, -0.04225076362490654, -0.02135610207915306, -0.006142943631857634, 0.005403915885835886, 0.011038552969694138, -0.0017421230440959334, -0.0354156568646431, -0.018198110163211823, -2.595328442112077e-05, -0.0011238630395382643, 0.012019116431474686, 0.018299050629138947, -0.018313471227884293, 0.001618651207536459, -0.025350458920001984, 0.0684664249420166, 0.01723196730017662, -0.024917855858802795, 0.02607146091759205, 0.022120365872979164, 0.006121313665062189, 0.0073902783915400505, -0.011831656098365784, -0.02754230611026287, -0.06315984576940536, -0.02415359392762184, -0.060045111924409866, -0.01863071136176586, -0.022077105939388275, -0.03965515270829201, 0.036165498197078705, -0.010072409175336361, -0.04103947803378105, -0.06010279431939125, 0.018428830429911613, -0.019813155755400658, 0.04121251776814461, 0.03835734724998474, -0.013763943687081337, 0.05975671112537384, -0.007714729756116867, 0.04856674745678902, 0.018082749098539352, -0.015169898979365826, -0.029388073831796646, -0.04796110466122627, 0.04793226718902588, -0.01198306679725647, 0.019668955355882645, 0.015977421775460243, -0.007336203474551439, -0.04461565241217613, -0.011673035100102425, -0.007534479256719351, 0.010988082736730576, -0.004992944188416004, -0.057449501007795334, -0.020678360015153885, 0.015241999179124832, -0.007563319057226181, -0.00038032897282391787, -0.00319944997318089, 0.03166644275188446, 0.1210131049156189, 0.007112692575901747, -0.041616279631853104, -0.001683541457168758, 0.036280859261751175, 0.024067072197794914, -0.0023144190199673176, 0.04992223158478737, -0.018313471227884293, -0.008998114615678787, 0.006096078548580408, -0.04548085480928421, 0.004899214021861553, -0.026057040318846703, -0.06229463964700699, 0.01178839523345232, -0.009055795148015022, -0.028061429038643837, -0.010281499475240707, -0.010158929042518139, -0.0159485824406147, 0.04089527949690819, -0.005962693132460117, 0.017405007034540176, 0.005126329604536295, 0.006892786826938391, 0.047095902264118195, 0.014268645085394382, -0.019481495022773743, -0.03411785140633583, -0.010043568909168243, -0.04657677933573723, 0.029229452833533287, -0.036511581391096115, -0.02692224457859993, 0.012437297962605953, -0.003574371337890625, -0.02809026837348938, -0.007995921187102795, -0.01606394164264202, 0.004012380726635456, -0.01883259229362011, -0.042193084955215454, -0.002620845101773739, -0.021716604009270668, 0.026778044179081917, -0.05583445727825165, -0.030858919024467468, -0.027109704911708832, -0.04931659251451492, 0.021759863942861557, -0.012747329659759998, 0.006972096860408783, -0.02855171076953411, -0.03322380781173706, 0.004390907008200884, 0.02708086557686329, 0.042856406420469284, 0.018818173557519913, -0.02057741954922676, -0.03558869659900665, -0.00951002724468708, -0.006687300745397806, -0.012826639227569103, 0.017361747100949287, 0.07948334515094757, -0.032704684883356094, -0.0014194742543622851, -0.017058925703167915, 0.04703822359442711, 0.020404379814863205, 0.0032679452560842037, -0.02816236950457096, 0.0012500386219471693, -0.005634636618196964, -0.003731189528480172, 0.00674858596175909, 0.04963383078575134, -0.015400619246065617, -0.0005488633760251105, 0.0006502543692477047, 0.003123744623735547, -0.054248251020908356, -0.04761502519249916, -0.0052993702702224255, -0.04917239025235176, -0.005832912400364876, -0.018183689564466476, -0.0009030560031533241, 0.01665516383945942, 0.008774603717029095, 0.018226951360702515, -0.008168961852788925, 0.01212005689740181, 0.019467074424028397, 0.008205011487007141, -0.020808139815926552, -0.02011597715318203, -0.01976989582180977, -0.032502803951501846, -0.008623193018138409, 0.01899121329188347, -0.019625695422291756, -0.019755477085709572, -0.015977421775460243, 0.001533933449536562, 0.0034518009051680565, -0.042856406420469284, -0.00465046800673008, -0.04127020016312599, 0.016583064571022987, 0.029849516227841377, -0.016193723306059837, -0.02552349865436554, 0.003900625044479966, -0.008666453883051872, -0.020678360015153885, -0.005631031934171915, -0.0013122251257300377, 0.0016727264737710357, 0.012062376365065575, 0.01645328290760517, 0.03120500035583973, 0.01638118363916874, 0.0038068946450948715, -0.024211274459958076, -0.013071781024336815, -0.022999988868832588, -0.0365692600607872, 0.008493413217365742, -0.004549527540802956, 0.00785893015563488, 0.018616292625665665, 0.032820045948028564, -0.027873968705534935, -0.018255790695548058, -0.02163008414208889, 0.009632597677409649, -0.038732271641492844, 0.007484009023755789, -0.01785202883183956, 0.040289636701345444, -0.014614726416766644, -0.02167334407567978, 0.004419747274369001, -0.021846383810043335, 0.008709713816642761, 0.024946697056293488, -0.02905641309916973, 0.01606394164264202, 0.004271941725164652, -0.005209245253354311, 0.013713473454117775, 0.003073274390771985, -0.020130397751927376, 0.011716295033693314, 0.00028457079315558076, -0.013742312788963318, -0.014203755185008049, 0.030743559822440147, -0.018414411693811417, 0.03408901020884514, 0.041847001761198044, 0.042856406420469284, 0.025148577988147736, -0.018385570496320724, -0.045192454010248184, 0.0032607351895421743, 0.015862060710787773, -0.00904858484864235, -0.004592787940055132, -0.01207679696381092, -0.016626324504613876, 0.030397478491067886, 0.01782318949699402, -0.014203755185008049, 0.005908617749810219, -0.001970140030607581, 0.03157992288470268, -0.02894105203449726, 0.013122250325977802, -0.004437772091478109, 0.01684262603521347, 0.03515609726309776, -0.003574371337890625, -0.03645390272140503, 0.05220060050487518, -0.02754230611026287, -0.011990276165306568, -0.025465818122029305, 0.01863071136176586, -0.02754230611026287, -0.0005317395552992821, -0.017405007034540176, 0.012559868395328522, 0.002957914024591446, 0.037520986050367355, -0.015040118247270584, -0.018255790695548058, -0.02447083406150341, -0.00303902686573565, 0.010901561938226223, 0.03581941872835159, -0.021716604009270668, 0.01743384823203087, 0.01692914590239525, -0.01451378595083952, -0.01642444357275963, -0.0094307167455554, -0.014924758113920689, -0.02594168111681938, 0.013518801890313625, 0.0034355781972408295, 0.01734732650220394, -0.009993098676204681, -0.011889335699379444, -0.002921863691881299, -0.005811282433569431, 0.03097428008913994, -0.018962373957037926, -0.03567521646618843, -0.027340425178408623, -0.01578996144235134, 0.041962362825870514, -0.008594353683292866, -0.016049522906541824, 0.009214415214955807, -0.0266482625156641, -0.03478117287158966, -0.04724010452628136, 0.0007575035560876131, -0.003713164245709777, -0.007628209423273802, 0.049431949853897095, -0.0074912188574671745, -0.04602881893515587, 0.016871465370059013, -0.06010279431939125, -0.031176161020994186, 0.003709559328854084, 0.048768628388643265, 0.01656864397227764, 0.06817802041769028, -0.00819059181958437, 0.022552967071533203, -0.009517236612737179, -0.019856417551636696, 0.02112538181245327, -0.009185575880110264, -0.012437297962605953, -0.020678360015153885, -0.03804010897874832, 0.028724750503897667, -0.04060687869787216, -0.04311596602201462, -0.028450770303606987, -0.03123384155333042, -0.04683634266257286, 0.030599357560276985, 0.015515980310738087, -0.04527897387742996, 0.014542626217007637, 0.02925829403102398, 0.015862060710787773, -0.007210027892142534, -0.019308455288410187, 0.013504382222890854, -0.004019590560346842, -0.00604200316593051, 0.010562690906226635, -0.03031095676124096, -0.00586535781621933, -0.009747957810759544, -0.007141532842069864, 0.016784945502877235, -0.026316601783037186, 0.020793721079826355, 0.006153758615255356, -0.0016574051696807146, 0.006593570578843355, -0.0037275843787938356, -0.011910965666174889, -0.011204383336007595, 0.0231441892683506, 0.017477108165621758, 0.0035004685632884502, 0.010346390306949615, 0.012220997363328934, 0.004708148073405027, 0.02155798301100731, -0.05375796929001808, -0.03264700621366501, 0.04103947803378105, 0.017030086368322372, 0.014614726416766644, -0.028220050036907196, 0.013035730458796024, 0.00031003120238892734, 0.007029777392745018, -0.01991409622132778, -0.006096078548580408, -0.0037708445452153683, -0.0016420838655903935, -0.023475850000977516, -0.023793091997504234, -0.028234468773007393, 0.023475850000977516, -0.0115720946341753, 0.024759234860539436, 0.010843882337212563, -0.010865512304008007, -0.0020170053467154503, -0.10215167701244354, -0.008385262452065945, -0.020649520680308342, -0.007134322542697191, 0.01093040220439434, 0.022999988868832588, 0.0033418480306863785, 0.05321000516414642, -0.012458927929401398, -0.01493917778134346, 0.041068319231271744, -0.03587709739804268, 0.012019116431474686, -0.0022531338036060333, 0.017765508964657784, -0.018198110163211823, 0.05854542553424835, -0.038616910576820374, -0.005598586518317461, 0.02813352830708027, -0.03452161327004433, -0.014254225417971611, 0.01968337595462799, 0.030051397159695625, 0.01285547949373722, -0.03700186312198639, -0.0318971648812294, 0.022077105939388275, 0.0004727074410766363, 0.04450029134750366, -0.003731189528480172, -0.009495606645941734, -0.02886895090341568, 0.006489024963229895, -0.007873350754380226, 0.04657677933573723, -0.0070514073595404625, -0.00986331794410944, -0.016525384038686752, 0.0038429449778050184, -0.023100929334759712, 0.006611595395952463, -0.025162996724247932, 0.03429089114069939, 0.007584949489682913, 0.01315109059214592, -0.029474593698978424, 0.002090908121317625, -0.0030606568325310946, 0.0050542294047772884, 0.010915982536971569, -0.009488396346569061, 0.0016565038822591305, -0.02607146091759205, -0.004055640660226345, -0.0029290737584233284, -0.002361284103244543, -0.020923500880599022, 0.02318744920194149, 0.010007518343627453, 0.017909709364175797, 0.0051623801700770855, 0.005018179304897785, -0.02206268534064293, -0.017058925703167915, -0.03224324434995651, -0.019784316420555115, 0.02976299449801445, -0.05142191797494888, 0.005706737283617258, -0.018919114023447037, 0.03157992288470268, 0.001268063671886921, 0.02229340560734272, -0.006651250645518303, -0.008515043184161186, 0.0018890273058786988, 0.02543697878718376, -0.005631031934171915, -0.014030714519321918, -0.00522366538643837, -0.015227578580379486, 0.0014510181499645114, 0.004679308272898197, -0.025970520451664925, 0.009589336812496185, 0.018111590296030045, -0.02077930048108101, -0.04438493028283119, 0.004405327141284943, 0.024946697056293488, 0.02174544334411621, -0.011305323801934719, 0.011247643269598484, 0.011867705732584, 0.04458681121468544, -0.00012730206071864814, 0.0016132437158375978, -0.00490281917154789, -0.022985568270087242, 0.043087124824523926, 0.04897050932049751, 0.010605950839817524, 0.022163625806570053, -0.014542626217007637, 0.021457042545080185, 0.03933791071176529, -0.016828205436468124, -0.0028479609172791243, -0.01062037143856287, 0.029734155163168907, -0.015299678780138493, -0.007635419722646475, 0.03581941872835159, -0.025004377588629723, 0.011918175965547562, 0.03457929193973541, 0.004311596509069204, 0.012949209660291672, -0.003756424644961953, 0.030483998358249664, -0.001825038343667984, -0.010980872437357903, -0.01899121329188347, 0.006092473398894072, -0.001815124531276524, 0.012999679893255234, 0.028508450835943222, -0.016006261110305786, -0.019164254888892174, -0.020562998950481415, -0.010094039142131805, 0.004412536974996328, 0.014362375251948833, 0.001353682717308402, -0.03175296261906624, 0.013374601490795612, 0.018760493025183678, -0.015905320644378662, 0.016323503106832504, 0.008630403317511082, -0.009077425114810467, 0.002757835667580366, -0.02563885971903801, 0.0013095212634652853, -0.00014521446428261697, -0.03155108168721199, 0.058372385799884796, -0.02447083406150341, 0.040520355105400085, 0.016943564638495445, 0.024759234860539436, -0.03414669260382652, 0.03310844674706459, -0.045538537204265594, 0.047759223729372025, 0.01097366213798523, -0.03163760155439377, 0.029272712767124176, -0.00478385342285037, 0.02524951845407486, 0.04426957294344902, -0.006622410845011473, -0.014823817647993565, -0.029907194897532463, -0.005843727383762598, -0.021716604009270668, 0.02462945505976677, 0.00203683297149837, -0.020678360015153885, -0.02442757412791252, -0.03457929193973541, 0.03749214485287666, -0.0010905166855081916, 0.012891530059278011, -0.017131026834249496, 0.022206885740160942, 0.02424011379480362, -0.0008579933200962842, -0.004199841059744358, -0.030512837693095207, 0.042885247617959976, -0.029907194897532463, 0.06794730573892593, -0.0031453745905309916, 0.021038861945271492, 0.030051397159695625, 0.001298706280067563, 0.02944575436413288, 0.018299050629138947, 0.006979307159781456, 0.03143572062253952, 0.00392946507781744, 0.015371779911220074, 0.02598494105041027, -0.03175296261906624, -0.015371779911220074, 0.0033706880640238523, 0.009135105647146702, -0.017924129962921143, 0.009596547111868858, 0.02428337372839451, 0.061025675386190414, 0.0007944549433887005, 0.01734732650220394, -0.00041637910180725157, -0.01093040220439434, -0.009740747511386871, 0.009322565980255604, 0.018587451428174973, -0.001128369360230863, -0.008154541254043579, -0.027686506509780884, 0.03824198618531227, -0.028263309970498085, -0.018731651827692986, -0.027109704911708832, 0.006860341411083937, -0.0027902808506041765, -0.02190406434237957, -0.008608773350715637, 0.0011247643269598484, 0.020995602011680603, -0.0011545057641342282, -0.021370522677898407, 0.019668955355882645, 0.04692286252975464, 0.00704059237614274, -0.006150153931230307, -0.005454386118799448, -0.01173071563243866, -0.008680873550474644, 0.055805616080760956, -0.005447176285088062, -0.01871723309159279, -0.04501941427588463, 0.02415359392762184, -0.05309464409947395, -0.0002426624996587634, 0.019582435488700867, 0.02571095898747444, 0.015314099378883839, 0.004524292424321175, -0.004084480926394463, -0.016986826434731483, 0.0073398081585764885, -0.013886514119803905, 0.02486017532646656, 0.002229701029136777, 0.0017538393149152398, -0.004982129205018282, -0.022538546472787857, 0.034175533801317215, -0.019640116021037102, 0.010591531172394753, 0.008212221786379814, 0.013684633187949657, 0.01863071136176586, 0.014391215518116951, 0.0014969820622354746, -0.0318971648812294, -0.0052561103366315365, 0.0012996075674891472, -0.002045845380052924, -0.03524261713027954, -0.0054327561520040035, -0.02341817133128643, 0.005468806251883507, 0.02687898464500904, -0.017952969297766685, 0.00961096677929163, 0.05023947358131409, 0.0005308383260853589, -0.0034373807720839977, 0.0020584629382938147, 0.04115483909845352, -0.0029543088749051094, -0.006561125162988901, -0.03925139084458351, -0.026013780385255814, -0.01871723309159279, 0.01578996144235134, -0.00514435488730669, 0.03296424821019173, 0.023562371730804443, -0.02610030025243759, -0.00964701734483242, -0.012999679893255234, 0.009755168110132217, -0.031839482486248016, -0.01194701623171568, 0.017751088365912437, -0.014174914918839931, 0.016784945502877235, -0.009365825913846493, 0.002519904635846615, -0.002510892227292061, 0.007527268957346678, 0.01700124517083168, 0.005360655952244997, 0.05208524316549301, 0.019712217152118683, 0.027008764445781708, 0.026907823979854584, -0.03607897832989693, -0.015515980310738087, -0.020043877884745598, -0.03157992288470268, 0.019265195354819298, 0.0052056401036679745, -0.015256418846547604, 0.04181816056370735, -0.014167704619467258, -0.01766456849873066, 0.03665577992796898, 0.003814104711636901, 0.026504062116146088, 0.005032599437981844, -0.002074685413390398, 0.029561113566160202, 0.004769433289766312, 0.029734155163168907, -0.0057031321339309216, -0.012235417030751705, -0.006352034397423267, -0.0115720946341753, -0.028220050036907196, -0.014290275052189827, 0.012696859426796436, -0.009993098676204681, 0.005302975419908762, 0.003044434357434511, -0.0008395176264457405, 0.0023883217945694923, -0.004066455643624067, 0.013504382222890854, -0.004690123256295919, -0.014838237315416336, -0.02139936201274395, -0.011002502404153347, 0.005832912400364876, -0.016395604237914085, -0.03342568874359131, 0.0003413497470319271, 0.01462914701551199, 0.011802815832197666, -0.028450770303606987, 0.020534159615635872, 0.042308442294597626, -0.0010238239774480462, 0.06050655245780945, -0.02174544334411621, -0.014571466483175755, -0.008450152352452278, 0.020447639748454094, 0.026590581983327866, -0.0004542317474260926, -0.00803197082132101, -0.014895917847752571, -0.0032679452560842037, -0.015544820576906204, 0.023821931332349777, 0.018573032692074776, -0.009920998476445675, 0.041414398699998856, -0.004956894088536501, 0.00028885173378512263, -0.010771781206130981, -0.02921503223478794, -0.015227578580379486, 0.02190406434237957, 0.029041992500424385, -0.00314717716537416, -0.033281490206718445, 0.008962064981460571, 0.0030047791078686714, -0.033829450607299805, 0.0006493531400337815, -0.061371758580207825, 0.040837597101926804, -0.0016655164072290063, -0.001614145003259182, 0.009618177078664303, -0.00048757813055999577, -0.015934161841869354, 0.012228207662701607, 0.01036802027374506, 0.0025325221940875053, -0.01743384823203087, 0.012617548927664757, -0.016121622174978256, -0.00028164172545075417, -0.022927889600396156, -0.02291346900165081, -0.01645328290760517, 0.013425071723759174, -0.016006261110305786, 0.025725379586219788, 0.006701720878481865, -0.012956419959664345, 0.003017396666109562, 0.005364260636270046, 0.010915982536971569, -0.010216609574854374, -0.0009598349570296705, 0.026403121650218964, -0.01902005448937416, 0.04527897387742996, 0.00489200372248888, 0.0005840122466906905, -0.02708086557686329, -0.015126638114452362, -0.01913541369140148, 0.04077991843223572, -0.05739182233810425, -0.006287144497036934, -0.02105328068137169, -0.02225014567375183, -0.011016923002898693, 0.014066764153540134, 0.03703070431947708, 0.015040118247270584, -0.01155767496675253, -0.003367082914337516, 0.007015357259660959, 0.03097428008913994, 0.027282744646072388, -0.03570405766367912, 0.0023198265116661787, -0.028335409238934517, -0.027095284312963486, 0.022971149533987045, 0.014066764153540134, 0.004762223456054926, 0.015299678780138493, -0.0013437689049169421, -0.022927889600396156, 0.041299041360616684, 0.013677422888576984, 0.0010003913193941116, 0.003875389927998185, -0.008817863650619984, 0.02178870514035225, 0.004243101458996534, -0.01062037143856287, -0.010512220673263073, -0.020865820348262787, -0.0017916918732225895, 0.024557355791330338, -0.0015014882665127516, 0.02882569096982479, 0.0021648108959198, -0.02933039329946041, 0.022322246804833412, -0.023389330133795738, -0.026504062116146088, -0.01349717192351818, -0.01656864397227764, -0.01832788996398449, 0.011427894234657288, 0.014420055784285069, 0.01704450510442257, 0.03022443689405918, -0.0305416788905859, 0.038184307515621185, -0.037059541791677475, -0.011442313902080059, 0.003062459407374263, -0.012776168994605541, 0.011211593635380268, 0.01011566910892725, 0.0121561074629426, 0.035732898861169815, -0.004228681325912476, 0.009077425114810467, -0.01847209222614765, 0.01863071136176586, -0.057795584201812744, 0.017722249031066895, -0.015862060710787773, 0.013425071723759174, -0.019265195354819298, 0.020635100081562996, 0.0011499994434416294, 0.0186739731580019, -0.0004249410121701658, -0.0013068175176158547, -0.021255161613225937, 0.014153284952044487, 0.02155798301100731, -0.0073470184579491615, 0.026129141449928284, -0.0026352652348577976, 0.021990584209561348, 0.022812528535723686, 0.03760750591754913, -0.006647645495831966, -0.03544449806213379, 0.0012590511469170451, -0.005090279504656792, 0.02197616547346115, -0.0025649673771113157, -0.0036717066541314125, 0.0020476479548960924, 0.0029056412167847157, -0.00550485635176301, -0.0072208428755402565, -0.022927889600396156, 0.02380751259624958, -0.014910337515175343, -0.026619423180818558, -0.005313790403306484, -0.008291532285511494, -0.01855861209332943, 0.021932905539870262, 0.00955328717827797, -0.0028407510835677385, 0.026835724711418152, -0.027801867574453354, -0.010980872437357903, -0.022740427404642105, -0.009063005447387695, 0.03625202178955078, -0.005263320170342922, 0.04213540256023407, -0.03899183124303818, -0.01268243882805109, 0.0072028180584311485, -0.02447083406150341, 0.011932596564292908, 0.003484246088191867, 0.03576174005866051, 0.012790589593350887, 0.0036428666207939386, -0.0027109705843031406, 0.02478807605803013, 0.013331341557204723, -0.0039835404604673386, -0.012567078694701195, 0.006067238282412291, 0.009762377478182316, 0.016554223373532295, -0.028912212699651718, 0.014160494320094585, -0.01075736153870821, 0.030628198757767677, -0.003799684811383486, 0.046201858669519424, 0.008962064981460571, 0.019625695422291756, -0.025667699053883553, 0.004877584055066109, -0.01860187202692032, -0.005764417350292206, -0.010771781206130981, -0.017030086368322372, -0.0009427111363038421, -0.022697167471051216, 0.01681378483772278, 0.004481032490730286, 0.012653598561882973, -0.01902005448937416, -0.04049151763319969, 0.05153727903962135, 0.04957615211606026, -0.005018179304897785, -0.0053246053867042065, -0.0039835404604673386, 0.015054537914693356, -0.06454417109489441, 0.04571157693862915, -0.004279151558876038, 0.004646862857043743, 0.009762377478182316, -0.018111590296030045, 0.054882731288671494, 0.009820058010518551, -0.002417161827906966, 0.018313471227884293, 0.011990276165306568, -0.013309711590409279, -0.020721619948744774, -0.03218556568026543, 0.017909709364175797, -0.002736205467954278, -0.016669584438204765, -0.019510336220264435, -0.04848022758960724, 0.021226322278380394, 0.004235891159623861, -0.0016222562408074737, 0.007808460388332605, 0.0083636324852705, -0.011002502404153347, 0.01404513418674469, 0.02412475273013115, 0.012559868395328522, -0.0023630866780877113, -0.004434166941791773, -0.0009003522573038936, 0.020332278683781624, 0.02582632005214691, 0.008356422185897827, 0.0033003902062773705, -0.01773666776716709, -0.024456415325403214, 0.008241062052547932, -0.002009795280173421, -0.02116864174604416, -0.026561742648482323, 0.01848651096224785, 0.005933852866292, -0.013814412988722324, -0.00814733188599348, -0.002096315613016486, 0.03971283510327339, 0.01718870736658573, 0.013886514119803905, 0.014463315717875957, 0.039424434304237366, -0.01878933236002922, 0.028854532167315483, -0.004376486875116825, 2.5756135073606856e-05, -0.021572403609752655, 0.0025144971441477537, -0.027008764445781708, -0.0003068768128287047, 0.025537919253110886, -0.014340745285153389, -0.0032445124816149473, 0.019193094223737717, 0.004015985410660505, -0.015515980310738087, -0.01532851904630661, 0.002141378354281187, 0.010663631372153759, 0.03994355350732803, -0.004563947673887014, -0.004686518106609583, -0.0017006653361022472, 0.008882754482328892, 0.003121942048892379, -0.012999679893255234, -0.020909080281853676, -0.013071781024336815, -0.0129347899928689, 0.027340425178408623, 0.006629620678722858, 0.025884000584483147, -0.016092782840132713, 0.003738399362191558, -0.016799364238977432, 0.0017186903860419989, 0.04914354905486107, 0.01874607242643833, 0.009546076878905296, 0.009351406246423721, -0.008688083849847317, 0.011384634301066399, 0.03605014085769653, 0.023821931332349777, -0.009156735613942146, -0.0027848733589053154, 0.015530399978160858, 0.015184318646788597, -0.02311534993350506, -0.0022891839034855366, -0.011420683935284615, 0.014895917847752571, -0.0021179455798119307, 0.0020278203301131725, -0.028623810037970543, -0.004015985410660505, 0.058574266731739044, -0.00414216099306941, -0.023490270599722862, -0.0256821196526289, 0.04718242213129997, 0.020332278683781624, 0.026936663314700127, 0.01527083944529295, 0.035732898861169815, -0.006716141011565924, 0.03299308940768242, 0.0054579912684857845, -0.01692914590239525, 0.0010283301817253232, 0.023374909535050392, -0.020418798550963402, -0.028393089771270752, 0.05179683864116669, 0.023014409467577934, 0.005818492732942104, -0.008781814016401768, 0.03224324434995651, -0.006932441610842943, 0.012595918960869312, -0.006103288382291794, -0.013035730458796024, 0.014311905018985271, 0.006528680212795734, -0.028234468773007393, -0.008738554082810879, -0.014542626217007637, 0.012365197762846947, 0.011132283136248589, 0.0018430633936077356, -0.015371779911220074, 0.001153604476712644, -0.01451378595083952, 0.0039078351110219955, 0.026287762448191643, -0.005645452067255974, -0.01472287718206644, -0.015919741243124008, -0.014448896050453186, -0.026446381583809853, 0.015054537914693356, -0.006763006094843149, -0.0007899486809037626, 0.03521377593278885, 0.017534788697957993, 0.014319115318357944, 0.0016078362241387367, 0.0005407520802691579, -0.013944193720817566, -0.011269273236393929, -0.011125072836875916, -0.015689020976424217, 0.006406109780073166, -0.027845127508044243, 0.013915353454649448, -0.02062067948281765, -0.010310339741408825, 0.001764654298312962, -0.00610689353197813, -0.02559559978544712, 0.006135733798146248]\n",
            "Vector of Size: 1536\n"
          ]
        }
      ],
      "source": [
        "query = \"Can LCEL help take code from the notebook to production?\"\n",
        "\n",
        "query_vector = embedding_model.embed_query(query)\n",
        "print(query_vector)\n",
        "print(f\"Vector of Size: {len(query_vector)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkJhyvgEt5Vt"
      },
      "source": [
        "Now, let's compare it against each existing chunk's embedding by using cosine similarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "vfyDZlpmfWYa"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "\n",
        "def cosine_similarity(vec_1, vec_2):\n",
        "  return np.dot(vec_1, vec_2) / (norm(vec_1) * norm(vec_2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwnJ0uYQt-G_",
        "outputId": "5ed2d13c-564c-4e2d-b06a-9f4e2e75d9d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LangChain Expression Language or LCEL is a declarative way to easily compose chains together. There are several benefits to writing chains in this manner (as opposed to writing normal code):\n",
            "\n",
            "Async, Batch, and Streaming Support Any chain constructed this way will automatically have full sync, async, batch, and streaming support. This makes it easy to prototype a chain in a Jupyter notebook using the sync interface, and then expose it as an async streaming interface.\n",
            "0.5372983191981081\n"
          ]
        }
      ],
      "source": [
        "max_similarity = -float('inf')\n",
        "closest_chunk = \"\"\n",
        "\n",
        "for chunk, chunk_vector in embeddings_dict.items():\n",
        "  cosine_similarity_score = cosine_similarity(chunk_vector, query_vector)\n",
        "\n",
        "  if cosine_similarity_score > max_similarity:\n",
        "    closest_chunk = chunk\n",
        "    max_similarity = cosine_similarity_score\n",
        "\n",
        "print(closest_chunk)\n",
        "print(max_similarity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDC7HS7iumN-"
      },
      "source": [
        "And we get the expected result, which is the passage that specifically mentions prototyping in a Jupyter Notebook!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPJexL1kuw45"
      },
      "source": [
        "### Creating a Retriever\n",
        "\n",
        "Now that we have an idea of how we're getting our most relevant information - let's see how we could create a pipeline that would automatically extract the closest chunk to our query and use it as context for our prompt!\n",
        "\n",
        "First, we'll wrap the above in a helper function!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "tnLpo26pu8-1"
      },
      "outputs": [],
      "source": [
        "def retrieve_context(query, embeddings_dict, embedding_model):\n",
        "  query_vector = embedding_model.embed_query(query)\n",
        "  max_similarity = -float('inf')\n",
        "  closest_chunk = \"\"\n",
        "\n",
        "  for chunk, chunk_vector in embeddings_dict.items():\n",
        "    cosine_similarity_score = cosine_similarity(chunk_vector, query_vector)\n",
        "\n",
        "    if cosine_similarity_score > max_similarity:\n",
        "      closest_chunk = chunk\n",
        "      max_similarity = cosine_similarity_score\n",
        "\n",
        "  return closest_chunk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytrINkVrvL1Q"
      },
      "source": [
        "Now, let's add it to our pipeline!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Q26pl1osvNmL"
      },
      "outputs": [],
      "source": [
        "def simple_rag(query, embeddings_dict, embedding_model, chat_chain):\n",
        "  context = retrieve_context(query, embeddings_dict, embedding_model)\n",
        "\n",
        "  response = chat_chain.invoke({\"query\" : query, \"context\" : context})\n",
        "\n",
        "  return_package = {\n",
        "      \"query\" : query,\n",
        "      \"response\" : response,\n",
        "      \"retriever_context\" : context\n",
        "  }\n",
        "\n",
        "  return return_package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHjbWxTAvtLS",
        "outputId": "86e4baf5-a3e4-44c7-ba85-19f7a1d852ba"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'query': 'Can LCEL help take code from the notebook to production?',\n",
              " 'response': AIMessage(content='Yes, LCEL can help take code from the notebook to production. Since any chain constructed with LCEL will automatically have full sync, async, batch, and streaming support, it makes it easy to prototype a chain in a Jupyter notebook using the sync interface and then expose it as an async streaming interface for production.', response_metadata={'token_usage': {'completion_tokens': 64, 'prompt_tokens': 152, 'total_tokens': 216}, 'model_name': 'gpt-4o', 'system_fingerprint': 'fp_319be4768e', 'finish_reason': 'stop', 'logprobs': None}, id='run-882113cc-e1b4-416c-b45b-2fe516bd9d5d-0', usage_metadata={'input_tokens': 152, 'output_tokens': 64, 'total_tokens': 216}),\n",
              " 'retriever_context': 'LangChain Expression Language or LCEL is a declarative way to easily compose chains together. There are several benefits to writing chains in this manner (as opposed to writing normal code):\\n\\nAsync, Batch, and Streaming Support Any chain constructed this way will automatically have full sync, async, batch, and streaming support. This makes it easy to prototype a chain in a Jupyter notebook using the sync interface, and then expose it as an async streaming interface.'}"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "simple_rag(\"Can LCEL help take code from the notebook to production?\", embeddings_dict, embedding_model, chat_chain)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cD2URVX3O3XJ"
      },
      "source": [
        "####â“ Question #3:\n",
        "\n",
        "What does LCEL do that makes it more reliable at scale?\n",
        "\n",
        "Answer: Retries and fallbacks which can be configured for any part of LCEL chain, make it more reliable at scale.\n",
        "> HINT: Use your newly created `simple_rag` to help you answer this question!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfYsBu45-0pL"
      },
      "source": [
        "# ðŸ¤ Breakout Room #2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cx2AOb-QHwJm"
      },
      "source": [
        "## Task #5: Create a Simple RAG Application Using Qdrant, OpenAI, and LCEL\n",
        "\n",
        "Now that we have a grasp on how LCEL works, and how we can use LangChain and OpenAI to interact with our data - let's step it up a notch and incorporate Qdrant!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYJB3IxEwpnh"
      },
      "source": [
        "## LangChain Powered RAG\n",
        "\n",
        "First and foremost, LangChain provides a convenient way to store our chunks and their embeddings.\n",
        "\n",
        "It's called a `VectorStore`!\n",
        "\n",
        "We'll be using Drant as our `VectorStore` today. You can read more about it [here](https://qdrant.tech/documentation/).\n",
        "\n",
        "Think of a `VectorStore` as a smart way to house your chunks and their associated embedding vectors. The implementation of the `VectorStore` also allows for smarter and more efficient search of our embedding vectors - as the method we used above would not scale well as we got into the millions of chunks.\n",
        "\n",
        "Otherwise, the process remains relatively similar under the hood!\n",
        "\n",
        "Let's use [Steve Jobs iPhone 2007 Presentation Introduction Speech PDF](https://singjupost.com/wp-content/uploads/2014/07/Steve-Jobs-iPhone-2007-Presentation-Full-Transcript.pdf) as our data today!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBErqPRgxgZR"
      },
      "source": [
        "### Data Collection\n",
        "\n",
        "We'll be leveraging the `PyMUPDFLoader` to load our PDF directly from the web!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "-AHA9L3Jxo3r"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import PyMuPDFLoader\n",
        "\n",
        "docs = PyMuPDFLoader(\"https://singjupost.com/wp-content/uploads/2014/07/Steve-Jobs-iPhone-2007-Presentation-Full-Transcript.pdf\").load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uH7ZPVJLx6Cn"
      },
      "source": [
        "### Chunking Our Documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsSCRQUSyBKT"
      },
      "source": [
        "Let's do the same process as we did before with our `RecursiveCharacterTextSplitter` - but this time we'll use ~200 tokens as our max chunk size!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "SzolG1FLx2f_"
      },
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 200,\n",
        "    chunk_overlap = 0,\n",
        "    length_function = tiktoken_len,\n",
        ")\n",
        "\n",
        "split_chunks = text_splitter.split_documents(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpV4f1eXyXVJ",
        "outputId": "4da462d2-e4db-4c89-b84c-2e4f85ae1c09"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "86"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(split_chunks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTJ60Ck6ybe_"
      },
      "source": [
        "Alright, now we have 516 ~200 token long documents.\n",
        "\n",
        "Let's verify the process worked as intended by checking our max document length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "950mB338yZR8",
        "outputId": "43302516-c252-4b3b-a723-75be9728dfbd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "197\n"
          ]
        }
      ],
      "source": [
        "max_chunk_length = 0\n",
        "\n",
        "for chunk in split_chunks:\n",
        "  max_chunk_length = max(max_chunk_length, tiktoken_len(chunk.page_content))\n",
        "\n",
        "print(max_chunk_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDt3RufQy1cP"
      },
      "source": [
        "Perfect! Now we can carry on to creating and storing our embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kocCe4zLy5qT"
      },
      "source": [
        "### Embeddings and Vector Storage\n",
        "\n",
        "We'll use the `text-embedding-3-small` embedding model again - and `Qdrant` to store all our embedding vectors for easy retrieval later!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "7M0X1eVlWPFf"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import Qdrant\n",
        "\n",
        "qdrant_vectorstore = Qdrant.from_documents(\n",
        "    split_chunks,\n",
        "    embedding_model,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"Steve Job's Speech\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-NDvjfzXhVp"
      },
      "source": [
        "Now let's set up our retriever, just as we saw before, but this time using LangChain's simple `as_retriever()` method!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "Edjx19YBXavZ"
      },
      "outputs": [],
      "source": [
        "qdrant_retriever = qdrant_vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1OM0DiYcOj-"
      },
      "source": [
        "#### Back to the Flow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7apXaEBzQai"
      },
      "source": [
        "We're ready to move to the next step!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMhcU37dzV5k"
      },
      "source": [
        "### Setting up our RAG\n",
        "\n",
        "We'll use the LCEL we touched on earlier to create a RAG chain.\n",
        "\n",
        "Let's think through each part:\n",
        "\n",
        "1. First we need to retrieve context\n",
        "2. We need to pipe that context to our model\n",
        "3. We need to parse that output\n",
        "\n",
        "Let's start by setting up our prompt again, just so it's fresh in our minds!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oatgDa7cOXWV"
      },
      "source": [
        "####ðŸ—ï¸ Activity #2:\n",
        "\n",
        "Complete the prompt so that your RAG application answers queries based on the context provided, but *does not* answer queries if the context is unrelated to the query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "TE5tick_YPJj"
      },
      "outputs": [],
      "source": [
        "RAG_PROMPT = \"\"\"\n",
        "CONTEXT:\n",
        "{context}\n",
        "\n",
        "QUERY:\n",
        "{question}\n",
        "\n",
        "Use prompt as the first source to derive an answer.\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZKadufhc2RL"
      },
      "source": [
        "#### Our RAG Chain\n",
        "\n",
        "Notice how we have a bit of a more complex chain this time - that's because we want to return our sources with the response.\n",
        "\n",
        "Let's break down the chain step-by-step:\n",
        "\n",
        "1. We invoke the chain with the `question` item. Notice how we only need to provide `question` since both the retreiver and the `\"question\"` object depend on it.\n",
        "  - We also chain our `\"question\"` into our `retriever`! This is what ultimately collects the context through Qdrant.\n",
        "2. We assign our collected context to a `RunnablePassthrough()` from the previous object. This is going to let us simply pass it through to the next step, but still allow us to run that section of the chain.\n",
        "3. We finally collect our response by chaining our prompt, which expects both a `\"question\"` and `\"context\"`, into our `llm`. We also, collect the `\"context\"` again so we can output it in the final response object.\n",
        "\n",
        "The key thing to keep in mind here is that we need to pass our context through *after* we've retrieved it - to populate the object in a way that doesn't require us to call it or try and use it for something else."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "VnGthXpzzo-R"
      },
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "\n",
        "retrieval_augmented_qa_chain = (\n",
        "    # INVOKE CHAIN WITH: {\"question\" : \"<<SOME USER QUESTION>>\"}\n",
        "    # \"question\" : populated by getting the value of the \"question\" key\n",
        "    # \"context\"  : populated by getting the value of the \"question\" key and chaining it into the base_retriever\n",
        "    {\"context\": itemgetter(\"question\") | qdrant_retriever, \"question\": itemgetter(\"question\")}\n",
        "    # \"context\"  : is assigned to a RunnablePassthrough object (will not be called or considered in the next step)\n",
        "    #              by getting the value of the \"context\" key from the previous step\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    # \"response\" : the \"context\" and \"question\" values are used to format our prompt object and then piped\n",
        "    #              into the LLM and stored in a key called \"response\"\n",
        "    # \"context\"  : populated by getting the value of the \"context\" key from the previous step\n",
        "    | {\"response\": rag_prompt | openai_chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0KAPrtFMtRd"
      },
      "source": [
        "Let's get a visual understanding of our chain!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ceq3JfVLM74-",
        "outputId": "1cbcd0a6-1bb6-497e-dbf1-6a0eb5f66353"
      },
      "outputs": [],
      "source": [
        "!pip install -qU grandalf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8ocIXNGMsue",
        "outputId": "60622d4c-87fd-4b02-ba18-ba2e1180efe8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                       +---------------------------------+                         \n",
            "                       | Parallel<context,question>Input |                         \n",
            "                       +---------------------------------+                         \n",
            "                           *****                   ****                            \n",
            "                        ***                            ****                        \n",
            "                     ***                                   ****                    \n",
            "+--------------------------------+                             **                  \n",
            "| Lambda(itemgetter('question')) |                              *                  \n",
            "+--------------------------------+                              *                  \n",
            "                 *                                              *                  \n",
            "                 *                                              *                  \n",
            "                 *                                              *                  \n",
            "     +----------------------+                   +--------------------------------+ \n",
            "     | VectorStoreRetriever |                   | Lambda(itemgetter('question')) | \n",
            "     +----------------------+                   +--------------------------------+ \n",
            "                           *****                   *****                           \n",
            "                                ***             ***                                \n",
            "                                   ***       ***                                   \n",
            "                       +----------------------------------+                        \n",
            "                       | Parallel<context,question>Output |                        \n",
            "                       +----------------------------------+                        \n",
            "                                         *                                         \n",
            "                                         *                                         \n",
            "                                         *                                         \n",
            "                            +------------------------+                             \n",
            "                            | Parallel<context>Input |                             \n",
            "                            +------------------------+                             \n",
            "                              ****               ****                              \n",
            "                           ***                       ***                           \n",
            "                         **                             **                         \n",
            "     +-------------------------------+              +-------------+                \n",
            "     | Lambda(itemgetter('context')) |              | Passthrough |                \n",
            "     +-------------------------------+              +-------------+                \n",
            "                              ****               ****                              \n",
            "                                  ***         ***                                  \n",
            "                                     **     **                                     \n",
            "                           +-------------------------+                             \n",
            "                           | Parallel<context>Output |                             \n",
            "                           +-------------------------+                             \n",
            "                                         *                                         \n",
            "                                         *                                         \n",
            "                                         *                                         \n",
            "                       +---------------------------------+                         \n",
            "                       | Parallel<response,context>Input |                         \n",
            "                       +---------------------------------+                         \n",
            "                             ***                  ****                             \n",
            "                         ****                         ***                          \n",
            "                       **                                ****                      \n",
            "         +--------------------+                              **                    \n",
            "         | ChatPromptTemplate |                               *                    \n",
            "         +--------------------+                               *                    \n",
            "                    *                                         *                    \n",
            "                    *                                         *                    \n",
            "                    *                                         *                    \n",
            "             +------------+                  +-------------------------------+     \n",
            "             | ChatOpenAI |                  | Lambda(itemgetter('context')) |     \n",
            "             +------------+**                +-------------------------------+     \n",
            "                             ***                  ***                              \n",
            "                                ****          ****                                 \n",
            "                                    **      **                                     \n",
            "                       +----------------------------------+                        \n",
            "                       | Parallel<response,context>Output |                        \n",
            "                       +----------------------------------+                        \n"
          ]
        }
      ],
      "source": [
        "print(retrieval_augmented_qa_chain.get_graph().draw_ascii())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bQVzN_eX1M2"
      },
      "source": [
        "Let's try another visual representation:\n",
        "\n",
        "![image](https://i.imgur.com/Ad31AhL.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0C5CFRHOxtB"
      },
      "source": [
        "Let's test our chain out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "JSDyVefDaue4"
      },
      "outputs": [],
      "source": [
        "response = retrieval_augmented_qa_chain.invoke({\"question\" : \"What is the most important thing about the iPhone?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "yfEAoG3HLC3J",
        "outputId": "cd2c86ee-ca37-481b-f2d2-2fd564855cd8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The most important thing about the iPhone, as emphasized in the provided excerpts, is its groundbreaking design and technology integration. Steve Jobs highlighted several key aspects:\\n\\n1. **Multi-Touch Screen:** The iPhone introduced a revolutionary multi-touch screen, enhancing user interaction.\\n2. **Miniaturization and Custom Silicon:** Significant miniaturization and custom silicon were used to achieve its advanced functionalities.\\n3. **Power Management and OSX Integration:** The iPhone runs OSX, an advanced operating system with capabilities like multi-tasking, best-in-class networking, power management, and security.\\n4. **Advanced Sensors and Desktop-Class Applications:** It includes advanced sensors and supports desktop-class applications, setting it apart from other mobile devices.\\n5. **Widescreen Video iPod:** It also functions as a widescreen video iPod, allowing users to touch their music.\\n\\nSteve Jobs summarized it as \"the ultimate digital device,\" capable of putting one\\'s life in their pocket due to its comprehensive and advanced technological features.'"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSZFdCM5LFoq",
        "outputId": "4121ebc7-3a23-4501-8984-5c7f7b81b7a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Context:\n",
            "page_content='of the art in every facet of this design. So let me just talk a little bit about it here. Weâ€™ve got\\nthe multi-touch screen. A first. Miniaturization, more than any weâ€™ve done before. A lot of\\ncustom silicon. Tremendous power management. OSX inside a mobile device. Featherweight\\nprecision enclosures. Three advanced sensors. Desktop class applications, and of course, the\\nwidescreen video iPod. Weâ€™ve been innovating like crazy for the last few years on this, and\\nwe filed for over 200 patents for all the inventions in iPhone, and we intend to protect them.\\nSo, a lot of high technology. I think weâ€™re advancing the state of the art in every aspect of\\nthis design. So iPhone is like having your life in your pocket. Itâ€™s the ultimate digital device.' metadata={'source': 'https://singjupost.com/wp-content/uploads/2014/07/Steve-Jobs-iPhone-2007-Presentation-Full-Transcript.pdf', 'file_path': 'https://singjupost.com/wp-content/uploads/2014/07/Steve-Jobs-iPhone-2007-Presentation-Full-Transcript.pdf', 'page': 17, 'total_pages': 22, 'format': 'PDF 1.4', 'title': 'Steve Jobs iPhone 2007 Presentation (Full Transcript)', 'author': 'Married Wildebeest', 'subject': '', 'keywords': '', 'creator': '', 'producer': 'mPDF 7.0.3', 'creationDate': \"20200415062431+00'00'\", 'modDate': \"20200415062431+00'00'\", 'trapped': '', '_id': '310c65b70ae94caabd8c487f6e457626', '_collection_name': \"Steve Job's Speech\"}\n",
            "----\n",
            "Context:\n",
            "page_content='whatâ€™s on any other phone. Now how do we do this? Well, we start with a strong foundation.\\niPhone runs OSX.\\nNow, why would we want to run such a sophisticated operating system on a mobile\\ndevice? Well, because itâ€™s got everything we need. Itâ€™s got multi-tasking. Itâ€™s got the best\\nnetworking. It already knows how to power manage. Weâ€™ve been doing this on mobile\\ncomputers for years. Itâ€™s got awesome security. And the right apps. Itâ€™s got everything from\\nCocoa and the graphics and itâ€™s got core animation built in and itâ€™s got the audio and video\\nthat OSX is famous for. Itâ€™s got all the stuff we want. And itâ€™s built right in to iPhone. And\\nthat has let us create desktop class applications and networking. Not the crippled stuff that\\nyou find on most phones. This is real, desktop-class applications.' metadata={'source': 'https://singjupost.com/wp-content/uploads/2014/07/Steve-Jobs-iPhone-2007-Presentation-Full-Transcript.pdf', 'file_path': 'https://singjupost.com/wp-content/uploads/2014/07/Steve-Jobs-iPhone-2007-Presentation-Full-Transcript.pdf', 'page': 2, 'total_pages': 22, 'format': 'PDF 1.4', 'title': 'Steve Jobs iPhone 2007 Presentation (Full Transcript)', 'author': 'Married Wildebeest', 'subject': '', 'keywords': '', 'creator': '', 'producer': 'mPDF 7.0.3', 'creationDate': \"20200415062431+00'00'\", 'modDate': \"20200415062431+00'00'\", 'trapped': '', '_id': '9fdd830e63c24148a0c40ce4d1d99677', '_collection_name': \"Steve Job's Speech\"}\n",
            "----\n",
            "Context:\n",
            "page_content='you think. They told me this, they said, You had me at scrolling. So, the iPhone with the\\nmost amazing iPod ever. You can now touch your music. So thatâ€™s the iPod.\\nNow, letâ€™s take a look at a revolutionary phone. We want to reinvent the phone. Now, whatâ€™s\\nthe killer app? The killer app is making calls! Itâ€™s amazing â€” itâ€™s amazing how hard it is to\\nmake calls on most phones. Most people actually dial them every time. Most people donâ€™t\\nhave very many numbers in their address book they use their recents as their address book.\\nRight? How many of you do that? I bet more than a few. So, we want to let you use contacts\\nlike never before. You can synch your iPhone with your PC or Mac and bring down all your\\ncontacts right into your phone. So youâ€™ve got everybodyâ€™s numbers with you at all times.' metadata={'source': 'https://singjupost.com/wp-content/uploads/2014/07/Steve-Jobs-iPhone-2007-Presentation-Full-Transcript.pdf', 'file_path': 'https://singjupost.com/wp-content/uploads/2014/07/Steve-Jobs-iPhone-2007-Presentation-Full-Transcript.pdf', 'page': 6, 'total_pages': 22, 'format': 'PDF 1.4', 'title': 'Steve Jobs iPhone 2007 Presentation (Full Transcript)', 'author': 'Married Wildebeest', 'subject': '', 'keywords': '', 'creator': '', 'producer': 'mPDF 7.0.3', 'creationDate': \"20200415062431+00'00'\", 'modDate': \"20200415062431+00'00'\", 'trapped': '', '_id': '5db2c6b236c94c63b113f75e60955a12', '_collection_name': \"Steve Job's Speech\"}\n",
            "----\n",
            "Context:\n",
            "page_content='you set up what you want synched to your iPhone. And itâ€™s just like an iPod. Charge and\\nsynch. So synch with iTunes.\\nThird thing I want to talk about a little is design. Weâ€™ve designed something wonderful for\\nyour hand, just wonderful. This is what it looks like. Itâ€™s got a three-and-a-half-inch screen\\non it. Itâ€™s really big. And, itâ€™s the highest-resolution screen weâ€™ve ever shipped. Itâ€™s 160\\npixels per inch. Highest weâ€™ve ever shipped. Itâ€™s gorgeous. And on the front, thereâ€™s only one\\nbutton down there. We call it the Home button. Takes you Home from wherever you are.\\nAnd thatâ€™s it.\\nLetâ€™s take a look at the side. Itâ€™s really thin. Itâ€™s thinner than any smartphone out there, at' metadata={'source': 'https://singjupost.com/wp-content/uploads/2014/07/Steve-Jobs-iPhone-2007-Presentation-Full-Transcript.pdf', 'file_path': 'https://singjupost.com/wp-content/uploads/2014/07/Steve-Jobs-iPhone-2007-Presentation-Full-Transcript.pdf', 'page': 3, 'total_pages': 22, 'format': 'PDF 1.4', 'title': 'Steve Jobs iPhone 2007 Presentation (Full Transcript)', 'author': 'Married Wildebeest', 'subject': '', 'keywords': '', 'creator': '', 'producer': 'mPDF 7.0.3', 'creationDate': \"20200415062431+00'00'\", 'modDate': \"20200415062431+00'00'\", 'trapped': '', '_id': '00d8e7b77bd648269441d1bb5f0a477f', '_collection_name': \"Steve Job's Speech\"}\n",
            "----\n"
          ]
        }
      ],
      "source": [
        "for context in response[\"context\"]:\n",
        "  print(\"Context:\")\n",
        "  print(context)\n",
        "  print(\"----\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nagiJ6l6noPL"
      },
      "source": [
        "Let's see if it can handle a query that is totally unrelated to the source documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "HOd2nJKZnsty"
      },
      "outputs": [],
      "source": [
        "response = retrieval_augmented_qa_chain.invoke({\"question\" : \"What is the airspeed velocity of an unladen swallow?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "TmLCKNGZLTh6",
        "outputId": "a5082ad5-4fb6-4938-f9d8-8fa606622bfd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The query \"What is the airspeed velocity of an unladen swallow?\" is a humorous reference to a famous line from the movie \"Monty Python and the Holy Grail.\" In the context of your documents, which pertain to Steve Jobs\\' 2007 iPhone presentation, there is no relevant information about the airspeed velocity of swallows.\\n\\nHowever, to address the original query in a light-hearted manner, here\\'s a typical response inspired by the movie:\\n\\nThe airspeed velocity of an unladen European Swallow is approximately 24 miles per hour or 11 meters per second. This is, of course, a rough estimate and varies based on several factors including the species of the swallow and environmental conditions.\\n\\nIf you have any further questions or need information related to Steve Jobs\\' presentation, feel free to ask!'"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvkSiGXrPMYw"
      },
      "source": [
        "####â“ Question #4:\n",
        "\n",
        "What key innovations did the iPhone introduce?\n",
        "\n",
        "> HINT: Use your RAG Chain to answer this question.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
